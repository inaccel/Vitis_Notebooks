{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from queue import Queue\n",
    "import cv2 as cv\n",
    "import inaccel.coral as inaccel\n",
    "import io\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_SIZE = 3\n",
    "req_num = 8\n",
    "queue_size = 16 # not necessary, but it helps with resource management\n",
    "scale_down = 16 # must be greater than 7\n",
    "input_video = 'data/bbb_sunflower_1080p_30fps_stereo_abl.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ndarrays for the constant parameters of the accelerator\n",
    "to help the scheduling we create one copy of each array for each request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1# Create ndarrays for the constant parameters of the accelerator\n",
    "Low_thresh = []\n",
    "High_thresh = []\n",
    "Shape = []\n",
    "for i in range(req_num):\n",
    "    # Create vectors holding thresholds and shape:\n",
    "    high_thresh = inaccel.ndarray((FILTER_SIZE * FILTER_SIZE), dtype = np.uint8)\n",
    "    low_thresh = inaccel.ndarray((FILTER_SIZE * FILTER_SIZE), dtype = np.uint8)\n",
    "    shape = inaccel.ndarray((FILTER_SIZE * FILTER_SIZE), dtype = np.uint8)\n",
    "\n",
    "    for i in range(FILTER_SIZE*FILTER_SIZE):\n",
    "        shape[i] = 1\n",
    "\n",
    "    # Define the low and high thresholds\n",
    "    # Want to grab 3 colors (Yellow, Green, Red) for the input image\n",
    "    low_thresh[0] = 22 # Lower boundary for Yellow\n",
    "    low_thresh[1] = 150\n",
    "    low_thresh[2] = 60\n",
    "\n",
    "    high_thresh[0] = 38 # Upper boundary for Yellow\n",
    "    high_thresh[1] = 255\n",
    "    high_thresh[2] = 255\n",
    "\n",
    "    low_thresh[3] = 38 # Lower boundary for Green\n",
    "    low_thresh[4] = 150\n",
    "    low_thresh[5] = 60\n",
    "\n",
    "    high_thresh[3] = 75 # Upper boundary for Green\n",
    "    high_thresh[4] = 255\n",
    "    high_thresh[5] = 255\n",
    "\n",
    "    low_thresh[6] = 160 # Lower boundary for Red\n",
    "    low_thresh[7] = 150\n",
    "    low_thresh[8] = 60\n",
    "\n",
    "    high_thresh[6] = 179 # Upper boundary for Red\n",
    "    high_thresh[7] = 255\n",
    "    high_thresh[8] = 255\n",
    "    \n",
    "    Low_thresh.append(low_thresh)\n",
    "    High_thresh.append(high_thresh)\n",
    "    Shape.append(shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function for each frame submits requests to coral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2# Submitter\n",
    "def submitter():\n",
    "    for i in range(frame_num):\n",
    "        # try to read a new frame\n",
    "        ret, frame = input_stream.read()\n",
    "        if (ret == False):\n",
    "            queue.put((None, None, None, None))\n",
    "            break\n",
    "\n",
    "        in_img = inaccel.array(frame, dtype = np.uint8)\n",
    "        # Allocate the memory for output images:\n",
    "        out_img = inaccel.ndarray((in_img.shape[0],in_img.shape[1]), dtype = np.uint8)\n",
    "        # describe a new request\n",
    "        # Create a request:\n",
    "        request = inaccel.request('com.xilinx.vitis.vision.colordetect')\n",
    "\n",
    "        # Set request arguments:\n",
    "        request.arg(in_img)\n",
    "        request.arg(Low_thresh[i%req_num])\n",
    "        request.arg(High_thresh[i%req_num])\n",
    "        request.arg(Shape[i%req_num])\n",
    "        request.arg(out_img)\n",
    "        request.arg(np.int32(in_img.shape[0]))\n",
    "        request.arg(np.int32(in_img.shape[1]))\n",
    "        # try to submit that request\n",
    "        try:\n",
    "            session = inaccel.submit(request)\n",
    "        except RuntimeError:\n",
    "            print(\"Cannot connect to the Docker daemon. Is InAccel Coral running?\")\n",
    "\n",
    "            queue.put((None, None, None, None))\n",
    "            break\n",
    "            \n",
    "        # push the session reference as well as the ndarrays to the queue\n",
    "        queue.put((session, in_img, out_img, frame))\n",
    "        \n",
    "        # update the queue status\n",
    "        queue_status.value = queue.qsize() / queue_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function waits the complition of the requests and shows the input and output images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3# Waiter\n",
    "def waiter():\n",
    "    for i in tqdm(range(frame_num)):\n",
    "        start = time.time() * 1000\n",
    "        \n",
    "        # try to pop from the queue\n",
    "        session, in_img, out_img, frame = queue.get()\n",
    "        if (session == None):\n",
    "            break\n",
    "        \n",
    "        # convert the array to an image and write it to the widget\n",
    "        frame_img = PIL.Image.fromarray(frame)\n",
    "        f = io.BytesIO()\n",
    "        frame_img.save(f, format = 'jpeg')\n",
    "        image_in.value = f.getvalue()\n",
    "        image_in.width = int(width / scale_down)\n",
    "        image_in.height = int(height / scale_down)\n",
    "        \n",
    "        # wait for the session to complete\n",
    "        inaccel.wait(session)\n",
    "        \n",
    "        # normalize the output array\n",
    "        disp_img = out_img.view(np.ndarray).astype(np.uint8)\n",
    "        disp_img = cv.resize(cv.cvtColor(disp_img, cv.COLOR_GRAY2BGR), (int(width / scale_down), int(height / scale_down)))\n",
    "\n",
    "        # convert the array to an image and write it to the widget\n",
    "        disp_img = PIL.Image.fromarray(disp_img)\n",
    "        f = io.BytesIO()\n",
    "        disp_img.save(f, format = 'jpeg')\n",
    "        image_out.value = f.getvalue()\n",
    "        image_out.width = int(width / scale_down)\n",
    "        image_out.height = int(height / scale_down)        \n",
    "        \n",
    "        \n",
    "        # explicitly delete the inaccel ndarrays\n",
    "        # this step is not necessary, as the will be garbage collected\n",
    "        # but it helps free earlier the corresponding device memory\n",
    "        del in_img\n",
    "        del out_img\n",
    "        \n",
    "        # update the queue status\n",
    "        queue.task_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a VideoCapture object and Start Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a VideoCapture object\n",
    "input_stream = cv.VideoCapture(input_video)\n",
    "if (input_stream.isOpened() == False): \n",
    "      print(\"Error while opening the input video stream\")\n",
    "\n",
    "# get the input's properties\n",
    "frame_num = int(input_stream.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "height = int(input_stream.get(cv.CAP_PROP_FRAME_HEIGHT) / 2)\n",
    "width  = int(input_stream.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "fps = int(input_stream.get(cv.CAP_PROP_FPS))\n",
    "\n",
    "print(\"Video Info:\")\n",
    "print(\"   Frames: \" + str(frame_num))\n",
    "print(\"   Dimensions: \" + str(height * 2) + \"x\" + str(width))\n",
    "print(\"   Framerate: \" + str(fps) + \" fps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the widgets\n",
    "image_in = widgets.Image()\n",
    "image_out = widgets.Image()\n",
    "\n",
    "queue_status = widgets.FloatProgress(description = 'Queue:', value = 0.0, min = 0.0, max = 1.0)\n",
    "images = [image_in, image_out]\n",
    "image_box = widgets.HBox(images)\n",
    "\n",
    "# initialize the queue\n",
    "queue = Queue(maxsize = queue_size)\n",
    "\n",
    "# initialize the two threads\n",
    "thread1 = threading.Thread(target = waiter)\n",
    "thread2 = threading.Thread(target = submitter)\n",
    "\n",
    "display(queue_status)\n",
    "display(image_box)\n",
    "\n",
    "# start the threads\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "# wait for the threads to complete\n",
    "queue.join()\n",
    "thread1.join()\n",
    "thread2.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
